{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, hidden_layers=1, hidden_units = [3], activation='relu'):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.As = [0 for _ in range(hidden_layers + 1)]\n",
    "        self.Zs = [0 for _ in range(hidden_layers + 1)]      \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.n = None\n",
    "        self.m = None\n",
    "        self.label_num = None\n",
    "    \n",
    "    def one_hot_encoding(self, data_y):\n",
    "        # Y is a vector: 1 x m\n",
    "        d = OrderedDict()\n",
    "        for i, val in enumerate(np.sort(np.unique(data_y))):\n",
    "            d[val] = i\n",
    "        d = dict(d)\n",
    "        n = len(d)\n",
    "        m = data_y.shape[0]\n",
    "        one_hot = np.zeros((n, m))\n",
    "        one_hot[[d[i] for i in data_y], np.arange(m)] = 1\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    def _weight_biases_init(self, label_num):\n",
    "        np.random.seed(1)\n",
    "        if self.hidden_layers != len(self.hidden_units):\n",
    "            raise Exception(\"Number of Hidden Layers and hidden units don't match\")\n",
    "        \n",
    "        c = self.n\n",
    "        for layer in range(self.hidden_layers):\n",
    "            \n",
    "            r = self.hidden_units[layer]\n",
    "            self.weights.append(np.random.randn(r,c) * .01)\n",
    "            self.biases.append(np.zeros((r,1)))\n",
    "            c = r\n",
    "        \n",
    "        if self.hidden_units:\n",
    "            self.weights.append(np.random.rand(label_num, self.hidden_units[-1]) * .01)\n",
    "        else:\n",
    "            self.weights.append(np.random.rand(label_num, self.n) * .01)\n",
    "        self.biases.append(np.zeros((label_num, 1)))\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def activation_function(self, Z):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid(Z)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu(Z)\n",
    "    \n",
    "    def activation_derivative(self, Z):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return np.multiply(Z, 1-Z)\n",
    "        elif self.activation == 'relu':\n",
    "            temp = np.copy(Z)\n",
    "            temp[temp < 0] = 0\n",
    "            temp[temp > 0] = 1\n",
    "            return temp\n",
    "        else:\n",
    "            raise Exception(\"Not a valid activation function\")\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        # overflow protection\n",
    "        c = np.max(Z)        \n",
    "        exp_a = np.exp(Z-c)\n",
    "        return exp_a / np.sum(exp_a)            \n",
    "        \n",
    "    def cost_function(self, A, one_hot_Y):\n",
    "\n",
    "        delta = 1e-7\n",
    "        cost = -np.mean(np.multiply(one_hot_Y, np.log(A + delta))) / self.m\n",
    "        return cost\n",
    "    \n",
    "    def forward_prop(self, A_1, layer):\n",
    "\n",
    "        Z = np.dot(self.weights[layer], A_1) + self.biases[layer]\n",
    "        self.Zs[layer] = Z\n",
    "        \n",
    "        A = None        \n",
    "        # if the last layer has more than two nodes then use softmax\n",
    "        if layer == len(self.weights) - 1:\n",
    "            if self.label_num > 2:\n",
    "                A = self.softmax(Z)\n",
    "            else:\n",
    "                A = self.sigmoid(Z)\n",
    "        else:\n",
    "            A = self.activation_function(Z)\n",
    "        \n",
    "        self.As[layer] = A\n",
    "        return A\n",
    "    \n",
    "    def back_prop(self, dZ, layer, data_x):\n",
    "        A_1 = np.transpose(self.As[layer-1])\n",
    "        if layer == 0:\n",
    "            A_1 = np.transpose(data_x)\n",
    "            \n",
    "        dW = np.dot(dZ, A_1) / self.m\n",
    "        dB = np.sum(dZ, axis = 1, keepdims=True) / self.m\n",
    "        dZ_1 = None\n",
    "        if layer != 0 :\n",
    "            dZ_1 = np.dot(np.transpose(self.weights[layer]), dZ) * self.activation_derivative(self.Zs[layer-1])\n",
    "\n",
    "        return dZ_1, dW, dB\n",
    "    \n",
    "    def train(self, data_x, data_y, learning_rate = .01, epoch=100):\n",
    "        \n",
    "        self.n = data_x.shape[0]   # number of input features\n",
    "        self.m = data_x.shape[1]   # number of input examples\n",
    "        \n",
    "        one_hot_Y = self.one_hot_encoding(data_y)\n",
    "        self.label_num = one_hot_Y.shape[0]        \n",
    "        self._weight_biases_init(one_hot_Y.shape[0])\n",
    "\n",
    "        # feed forward\n",
    "        for step in range(1, epoch):\n",
    "            A = data_x\n",
    "            # iterating through layers\n",
    "            for layer in range(len(self.weights)):\n",
    "                A = self.forward_prop(A, layer)\n",
    "            \n",
    "            # cost\n",
    "            cost = self.cost_function(A, one_hot_Y)\n",
    "            print(step, cost)\n",
    "            \n",
    "            # back prop\n",
    "            dZ = (A - one_hot_Y)\n",
    "            for layer in range(len(self.weights)-1, -1, -1):\n",
    "                dZ, dW, dB = self.back_prop(dZ, layer, data_x)\n",
    "                \n",
    "                self.weights[layer] = (self.weights[layer] - learning_rate * dW)\n",
    "                self.biases[layer] = (self.biases[layer] - learning_rate * dB)\n",
    "                \n",
    "    def test(self, test_x, test_y):\n",
    "        \n",
    "        one_hot_Y = self.one_hot_encoding(test_y)\n",
    "        \n",
    "        A = test_x\n",
    "        for layer in range(self.hidden_layers):\n",
    "            Z = np.dot(self.weights[layer], A) + self.biases[layer]\n",
    "            A = self.activation_function(Z)\n",
    "            \n",
    "        Z = np.dot( self.weights[-1], A) + self.biases[-1]\n",
    "        # no needs to run an activation function for the last layer.\n",
    "        # argmax will figure it out\n",
    "        print(self.sigmoid(Z))\n",
    "\n",
    "        accuracy = np.mean(np.equal(np.argmax(Z, axis = 0)+1, test_y))\n",
    "        print(accuracy)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
